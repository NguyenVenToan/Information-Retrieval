import os
import requests
import zipfile
import streamlit as st
import numpy as np
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel, AutoProcessor, AutoModel
from sklearn.metrics.pairwise import cosine_similarity

# H√†m t·∫£i t·ªáp ZIP t·ª´ Hugging Face
def download_and_extract_zip(url, extract_to="images"):
    zip_path = "images.zip"
    response = requests.get(url, stream=True)
    with open(zip_path, "wb") as f:
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                f.write(chunk)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
    os.remove(zip_path)
    return extract_to

# Load CLIP model
@st.cache_resource
def load_clip_model():
    model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
    return model, processor

# Load ALIGN model
@st.cache_resource
def load_align_model():
    model = AutoModel.from_pretrained("kakaobrain/align-base")
    processor = AutoProcessor.from_pretrained("kakaobrain/align-base")
    return model, processor

# H√†m t√¨m ki·∫øm ·∫£nh
def search_image(text_query, model, processor, text_embeddings, image_embeddings, image_paths, top_k=5):
    inputs = processor(text=[text_query], return_tensors="pt", padding=True)
    with torch.no_grad():
        text_features = model.get_text_features(**inputs).numpy()

    similarities = cosine_similarity(text_features, image_embeddings)[0]
    top_k_indices = similarities.argsort()[-top_k:][::-1]

    result_images = [image_paths[i] for i in top_k_indices]
    return result_images

# Load embeddings v√† ƒë∆∞·ªùng d·∫´n ·∫£nh
@st.cache_data
def load_data(model_name, image_folder):
    if model_name == "CLIP":
        text_embeddings = np.load("text_embeddings.npy")
        image_embeddings = np.load("image_embeddings.npy")
    elif model_name == "ALIGN":
        text_embeddings = np.load("text_embeddings_align.npy")
        image_embeddings = np.load("image_embeddings_align.npy")

    image_filenames = np.load("image_filenames.npy", allow_pickle=True)
    image_paths = [os.path.join(image_folder, filename) for filename in image_filenames]

    return text_embeddings, image_embeddings, image_paths

# Giao di·ªán Streamlit
st.set_page_config(page_title="Image Search", page_icon="üì∑", layout="wide")
st.title("üì∑ Image Search with CLIP and ALIGN")

# T·∫£i v√† gi·∫£i n√©n ·∫£nh t·ª´ Hugging Face
zip_url = "https://huggingface.co/datasets/Lippovn04/images/resolve/main/images.zip"
image_folder = download_and_extract_zip(zip_url)

# Giao di·ªán
model_name = st.selectbox("Ch·ªçn model:", ["CLIP", "ALIGN"])
text_query = st.text_input("Nh·∫≠p m√¥ t·∫£ vƒÉn b·∫£n:")
top_k = st.slider("S·ªë l∆∞·ª£ng ·∫£nh mu·ªën t√¨m ki·∫øm:", min_value=1, max_value=20, value=5)

if st.button("T√¨m ki·∫øm"):
    if text_query:
        if model_name == "CLIP":
            model, processor = load_clip_model()
        elif model_name == "ALIGN":
            model, processor = load_align_model()

        text_embeddings, image_embeddings, image_paths = load_data(model_name, image_folder)

        result_images = search_image(text_query, model, processor, text_embeddings, image_embeddings, image_paths, top_k=top_k)

        st.markdown("### üéØ K·∫øt qu·∫£ t√¨m ki·∫øm")
        cols = st.columns(3)
        for i, img_path in enumerate(result_images):
            col = cols[i % 3]
            try:
                with col:
                    st.image(Image.open(img_path), caption=os.path.basename(img_path), use_container_width=True)
            except FileNotFoundError:
                st.error(f"Kh√¥ng t√¨m th·∫•y ·∫£nh: {img_path}")
    else:
        st.warning("Vui l√≤ng nh·∫≠p n·ªôi dung!")
