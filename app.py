import os
import requests
import zipfile
import streamlit as st
import numpy as np
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel, AutoProcessor, AutoModel
from sklearn.metrics.pairwise import cosine_similarity

# H√†m t·∫£i t·ªáp t·ª´ URL
def download_file(url, save_path):
    """T·∫£i t·ªáp t·ª´ URL v√† l∆∞u v√†o ƒë∆∞·ªùng d·∫´n c·ª•c b·ªô."""
    response = requests.get(url, stream=True)
    with open(save_path, "wb") as f:
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                f.write(chunk)

# H√†m t·∫£i v√† gi·∫£i n√©n t·ªáp ZIP t·ª´ng ph·∫ßn
def download_and_extract_zip(url, extract_to="images"):
    zip_path = "images.zip"
    response = requests.get(url, stream=True)
    with open(zip_path, "wb") as f:
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                f.write(chunk)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Gi·∫£i n√©n t·ªáp theo t·ª´ng ph·∫ßn thay v√¨ to√†n b·ªô m·ªôt l√∫c
        for file in zip_ref.namelist():
            zip_ref.extract(file, extract_to)
            print(f"ƒê√£ gi·∫£i n√©n: {file}")
    return extract_to

# H√†m t·∫£i embeddings t·ª´ Hugging Face
def load_embeddings_from_huggingface():
    hf_base_url = "https://huggingface.co/datasets/Lippovn04/Embeddings/resolve/main/"
    files = {
        "text_embeddings": "text_embeddings.npy",
        "text_embeddings_align": "text_embeddings_align.npy",
        "image_embeddings": "image_embeddings.npy",
        "image_embeddings_align": "image_embeddings_align.npy",
        "image_filenames": "image_filenames.npy",
    }
    for key, filename in files.items():
        if not os.path.exists(filename):
            download_file(hf_base_url + filename, filename)
            print(f"T·∫£i xong: {filename}")
        else:
            print(f"File {filename} ƒë√£ t·ªìn t·∫°i, kh√¥ng c·∫ßn t·∫£i l·∫°i.")

# Load CLIP model
@st.cache_resource
def load_clip_model():
    model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
    return model, processor

# Load ALIGN model
@st.cache_resource
def load_align_model():
    model = AutoModel.from_pretrained("kakaobrain/align-base")
    processor = AutoProcessor.from_pretrained("kakaobrain/align-base")
    return model, processor

# H√†m t√¨m ki·∫øm ·∫£nh
def search_image(text_query, model, processor, text_embeddings, image_embeddings, image_paths, top_k=5):
    inputs = processor(text=[text_query], return_tensors="pt", padding=True)
    with torch.no_grad():
        text_features = model.get_text_features(**inputs).numpy()

    similarities = cosine_similarity(text_features, image_embeddings)[0]
    top_k_indices = similarities.argsort()[-top_k:][::-1]

    result_images = [image_paths[i] for i in top_k_indices]
    return result_images

# Load embeddings v√† ƒë∆∞·ªùng d·∫´n ·∫£nh
@st.cache_data
def load_data(model_name, image_folder):
    if model_name == "CLIP":
        text_embeddings = np.load("text_embeddings.npy")
        image_embeddings = np.load("image_embeddings.npy")
    elif model_name == "ALIGN":
        text_embeddings = np.load("text_embeddings_align.npy")
        image_embeddings = np.load("image_embeddings_align.npy")

    image_filenames = np.load("image_filenames.npy", allow_pickle=True)
    image_paths = [os.path.join(image_folder, filename) for filename in image_filenames]

    return text_embeddings, image_embeddings, image_paths

# T·∫£i embeddings t·ª´ Hugging Face
load_embeddings_from_huggingface()
zip_url = "https://huggingface.co/datasets/Lippovn04/images/resolve/main/images.zip"
image_folder = download_and_extract_zip(zip_url)

# Giao di·ªán Streamlit
st.set_page_config(page_title="Image Search", page_icon="üì∑", layout="wide")
st.title("üì∑ Image Search with CLIP and ALIGN")
st.write("Nh·∫≠p m√¥ t·∫£ vƒÉn b·∫£n v√† t√¨m ki·∫øm ·∫£nh ph√π h·ª£p!")

# L·ª±a ch·ªçn model
model_name = st.selectbox("Ch·ªçn model:", ["CLIP", "ALIGN"])

# Nh·∫≠p li·ªáu
text_query = st.text_input("Nh·∫≠p m√¥ t·∫£ vƒÉn b·∫£n:")
top_k = st.slider("S·ªë l∆∞·ª£ng ·∫£nh mu·ªën t√¨m ki·∫øm:", min_value=1, max_value=20, value=5)

if st.button("T√¨m ki·∫øm"):
    if text_query:
        if model_name == "CLIP":
            model, processor = load_clip_model()
        elif model_name == "ALIGN":
            model, processor = load_align_model()

        text_embeddings, image_embeddings, image_paths = load_data(model_name, image_folder)

        result_images = search_image(text_query, model, processor, text_embeddings, image_embeddings, image_paths, top_k=top_k)

        st.markdown("### üéØ K·∫øt qu·∫£ t√¨m ki·∫øm")
        cols = st.columns(3)
        for i, img_path in enumerate(result_images):
            col = cols[i % 3]
            try:
                with col:
                    img = Image.open(img_path)
                    st.image(img, caption=os.path.basename(img_path), use_container_width=True)
            except FileNotFoundError:
                st.error(f"Kh√¥ng t√¨m th·∫•y ·∫£nh: {img_path}")
            except Exception as e:
                st.error(f"L·ªói khi t·∫£i ·∫£nh: {str(e)}")
    else:
        st.warning("Vui l√≤ng nh·∫≠p n·ªôi dung!")
